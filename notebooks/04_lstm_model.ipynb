{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tesla Stock Price Prediction - LSTM Model\n",
        "\n",
        "## Objective\n",
        "- Build LSTM models for 1-day, 5-day, and 10-day predictions\n",
        "- Use GridSearchCV (via Keras Tuner) for hyperparameter tuning\n",
        "- Tune: LSTM units, dropout rate, learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup Notes\n",
        "This notebook uses **PyTorch** for better Windows compatibility. Install torch if not already available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing PyTorch (better Windows support)...\n",
            "PyTorch installed successfully\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "print(\"Installing PyTorch (better Windows support)...\")\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"], timeout=300)\n",
        "    print(\"PyTorch installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"PyTorch install attempted: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "PROJECT_ROOT = Path('..').resolve()\n",
        "DATA_PATH = PROJECT_ROOT / 'data' / 'processed'\n",
        "MODELS_PATH = PROJECT_ROOT / 'models'\n",
        "MODELS_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, output_size=1, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        predictions = self.fc(lstm_out[:, -1, :])\n",
        "        return predictions\n",
        "\n",
        "def build_lstm_model(units=64, dropout=0.2):\n",
        "    return LSTMModel(input_size=1, hidden_size=units, output_size=1, dropout=dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameter tuning (grid search on 1-day data)...\n",
            "  Units: 32, Dropout: 0.1, LR: 0.001 → Val Loss: 0.000190\n",
            "  Units: 32, Dropout: 0.1, LR: 0.0005 → Val Loss: 0.000167\n",
            "  Units: 32, Dropout: 0.2, LR: 0.001 → Val Loss: 0.000136\n",
            "  Units: 64, Dropout: 0.1, LR: 0.001 → Val Loss: 0.000118\n",
            "  Units: 64, Dropout: 0.2, LR: 0.001 → Val Loss: 0.000110\n",
            "\n",
            "Best params: LSTM units=64, Dropout=0.2, LR=0.001\n"
          ]
        }
      ],
      "source": [
        "# Simple grid search for hyperparameter tuning on 1-day data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "X_train = np.load(DATA_PATH / 'X_train_1d.npy')\n",
        "y_train = np.load(DATA_PATH / 'y_train_1d.npy')\n",
        "X_test = np.load(DATA_PATH / 'X_test_1d.npy')\n",
        "y_test = np.load(DATA_PATH / 'y_test_1d.npy')\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
        "\n",
        "# Grid search parameters\n",
        "lstm_units_options = [32, 64, 96]\n",
        "dropout_options = [0.1, 0.2, 0.3]\n",
        "lr_options = [0.001, 0.0005]\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "print(\"Hyperparameter tuning (grid search on 1-day data)...\")\n",
        "for units in lstm_units_options:\n",
        "    for dropout in dropout_options:\n",
        "        for lr in lr_options:\n",
        "            model = build_lstm_model(units=units, dropout=dropout).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            criterion = nn.MSELoss()\n",
        "            \n",
        "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "            \n",
        "            # Train for limited epochs to find best params\n",
        "            for epoch in range(20):\n",
        "                model.train()\n",
        "                total_loss = 0\n",
        "                for X_batch, y_batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(X_batch)\n",
        "                    loss = criterion(outputs, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                \n",
        "                # Validation\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_train_tensor)\n",
        "                    val_loss = criterion(val_outputs, y_train_tensor).item()\n",
        "            \n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_params = {'units': units, 'dropout': dropout, 'lr': lr}\n",
        "                print(f\"  Units: {units}, Dropout: {dropout}, LR: {lr} → Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "print(f\"\\nBest params: LSTM units={best_params['units']}, Dropout={best_params['dropout']}, LR={best_params['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training 1-day LSTM ===\n",
            "Epoch 10/100, Train Loss: 0.000398, Val Loss: 0.000152\n",
            "Epoch 20/100, Train Loss: 0.000363, Val Loss: 0.000121\n",
            "Epoch 30/100, Train Loss: 0.000258, Val Loss: 0.000115\n",
            "Epoch 40/100, Train Loss: 0.000203, Val Loss: 0.000092\n",
            "Epoch 50/100, Train Loss: 0.000193, Val Loss: 0.000081\n",
            "Epoch 60/100, Train Loss: 0.000191, Val Loss: 0.000082\n",
            "Epoch 70/100, Train Loss: 0.000171, Val Loss: 0.000066\n",
            "Epoch 80/100, Train Loss: 0.000165, Val Loss: 0.000071\n",
            "Epoch 90/100, Train Loss: 0.000162, Val Loss: 0.000067\n",
            "Epoch 100/100, Train Loss: 0.000152, Val Loss: 0.000077\n",
            "1-day LSTM MSE: 0.000330\n",
            "\n",
            "\n",
            "=== Training 5-day LSTM ===\n",
            "Epoch 10/100, Train Loss: 0.000734, Val Loss: 0.000299\n",
            "Epoch 20/100, Train Loss: 0.000564, Val Loss: 0.000282\n",
            "Epoch 30/100, Train Loss: 0.000501, Val Loss: 0.000281\n",
            "Epoch 40/100, Train Loss: 0.000430, Val Loss: 0.000268\n",
            "Epoch 50/100, Train Loss: 0.000389, Val Loss: 0.000261\n",
            "Epoch 60/100, Train Loss: 0.000392, Val Loss: 0.000252\n",
            "Epoch 70/100, Train Loss: 0.000373, Val Loss: 0.000260\n",
            "Epoch 80/100, Train Loss: 0.000349, Val Loss: 0.000237\n",
            "Epoch 90/100, Train Loss: 0.000331, Val Loss: 0.000235\n",
            "Epoch 100/100, Train Loss: 0.000308, Val Loss: 0.000228\n",
            "5-day LSTM MSE: 0.001327\n",
            "\n",
            "\n",
            "=== Training 10-day LSTM ===\n",
            "Epoch 10/100, Train Loss: 0.001158, Val Loss: 0.000551\n",
            "Epoch 20/100, Train Loss: 0.000935, Val Loss: 0.000533\n",
            "Epoch 30/100, Train Loss: 0.000817, Val Loss: 0.000575\n",
            "Epoch 40/100, Train Loss: 0.000736, Val Loss: 0.000549\n",
            "Epoch 50/100, Train Loss: 0.000689, Val Loss: 0.000454\n",
            "Epoch 60/100, Train Loss: 0.000665, Val Loss: 0.000457\n",
            "Early stopping at epoch 69\n",
            "10-day LSTM MSE: 0.002643\n",
            "\n",
            "==================================================\n",
            "Final Results: [{'horizon': 1, 'mse': 0.00032957449754243023}, {'horizon': 5, 'mse': 0.001326954848287257}, {'horizon': 10, 'mse': 0.002643494020363522}]\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Train LSTM models for all horizons with best tuned params\n",
        "horizons = [1, 5, 10]\n",
        "results = []\n",
        "\n",
        "for horizon in horizons:\n",
        "    print(f\"\\n=== Training {horizon}-day LSTM ===\")\n",
        "    \n",
        "    # Load data\n",
        "    X_train = np.load(DATA_PATH / f'X_train_{horizon}d.npy')\n",
        "    y_train = np.load(DATA_PATH / f'y_train_{horizon}d.npy')\n",
        "    X_test = np.load(DATA_PATH / f'X_test_{horizon}d.npy')\n",
        "    y_test = np.load(DATA_PATH / f'y_test_{horizon}d.npy')\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1).to(device)\n",
        "    \n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    \n",
        "    # Build model with best params\n",
        "    model = build_lstm_model(units=best_params['units'], dropout=best_params['dropout']).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # Training loop with early stopping\n",
        "    epochs = 100\n",
        "    patience = 10\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_train_tensor)\n",
        "            val_loss = criterion(val_outputs, y_train_tensor).item()\n",
        "        \n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), MODELS_PATH / f'lstm_{horizon}day_best.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test_tensor).cpu().numpy()\n",
        "    \n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    results.append({'horizon': horizon, 'mse': mse})\n",
        "    print(f\"{horizon}-day LSTM MSE: {mse:.6f}\\n\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Final Results:\", results)\n",
        "print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
